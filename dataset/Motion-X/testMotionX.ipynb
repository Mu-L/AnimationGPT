{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-X\n",
    "face_motion_data: shape = (frames, 153)<br>\n",
    "motion_data: shape = (frames, 322)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## face_motion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_motion_data1 = np.load('face_motion_data/000000_clip0000.npy')\n",
    "face_motion_data2 = np.load('face_motion_data/000.npy')\n",
    "face_motion_data3 = np.load('face_motion_data/001.npy')\n",
    "face_motion_data4 = np.load('face_motion_data/000002_clip0000.npy')\n",
    "face_motion_data5 = np.load('face_motion_data/airplane_pass_1.npy')\n",
    "face_motion_data6 = np.load('face_motion_data/gamecontroller_play_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 153)\n",
      "(450, 153)\n",
      "(401, 153)\n",
      "(69, 153)\n",
      "(149, 153)\n",
      "(146, 153)\n"
     ]
    }
   ],
   "source": [
    "print(face_motion_data1.shape)\n",
    "print(face_motion_data2.shape)\n",
    "print(face_motion_data3.shape)\n",
    "print(face_motion_data4.shape)\n",
    "print(face_motion_data5.shape)\n",
    "print(face_motion_data6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## motion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data1 = np.load('motion_data/32_Form_Tai_Chi_Demonstration_Master_Form3_Single_Whip_Left.npy')\n",
    "motion_data2 = np.load('motion_data/32_Form_Tai_Chi_Demonstration_Master_Form3_Single_Whip_Left2.npy')\n",
    "motion_data3 = np.load('motion_data/Dance_Break_3_Step.npy')\n",
    "motion_data4 = np.load('motion_data/Dance_Krump_Arm_Swing_clip_1.npy')\n",
    "motion_data5 = np.load('motion_data/Ways_To_Catch_360.npy')\n",
    "motion_data6 = np.load('motion_data/Ways_To_Wake_Up_Fall_Out_Of_Bed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 322)\n",
      "(10, 322)\n",
      "(82, 322)\n",
      "(300, 322)\n",
      "(117, 322)\n",
      "(37, 322)\n"
     ]
    }
   ],
   "source": [
    "print(motion_data1.shape)\n",
    "print(motion_data2.shape)\n",
    "print(motion_data3.shape)\n",
    "print(motion_data4.shape)\n",
    "print(motion_data5.shape)\n",
    "print(motion_data6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumanML3D & soul_v3\n",
    "new_joint_vecs_data: shape = (frames, 263)<br>\n",
    "263 is motion features extracted using motion_representation.ipynb<br>\n",
    "more infomation about dim-263: https://github.com/EricGuo5513/HumanML3D/issues/26\n",
    "<br>\n",
    "new_joints_data: shape = (frames, 22, 3)<br>\n",
    "<br>\n",
    "\n",
    "[bs, seqlen, 263/251] HumanML/KIT<br>\n",
    "- ./new_joint_vecs.rar //从3D运动位置提取旋转不变特征和旋转特征向量。\n",
    "- ./new_joints.rar //3D 运动位置。\n",
    "- ./Mean.npy //new_joint_vecs中所有数据的平均值\n",
    "- ./Std.npy //new_joint_vecs中所有数据的标准差\n",
    "<br>\n",
    "\n",
    "**SMPL有23个关节和一个根关节，而 HumanML3D 有 22 个关节：SMPL 的前 22 个关节（第23和第24关节是手关节）**<br>\n",
    "- root_rot_velocity (B, seq_len, 1) = 1\n",
    "- root_linear_velocity (B, seq_len,2) = 2\n",
    "- root_y (B, seq_len, 1) = 1\n",
    "- ric_data (B, seq_len, (joint_num - 1)*3) = 63\n",
    "- rot_data (B, seq_len, (joint_num - 1)*6) = 126\n",
    "- local_velocity (B, seq_len,joint_num*3) = 66\n",
    "- foot contact (B, seq_len, 4) = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_joint_vecs_data = np.load('..\\\\..\\\\algorithm\\HumanML3D\\HumanML3D\\\\new_joint_vecs\\\\012314.npy')\n",
    "new_joints_data = np.load('..\\\\..\\\\algorithm\\HumanML3D\\HumanML3D\\\\new_joints\\\\012314.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 263)\n",
      "(170, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "print(new_joint_vecs_data.shape)\n",
    "print(new_joints_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_joint_vecs_data = np.load('soul_v3\\\\new_joint_vecs\\SKR_250_600361.npy')\n",
    "new_joints_data = np.load('soul_v3\\\\new_joints\\SKR_250_600361.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169, 263)\n",
      "(169, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "print(new_joint_vecs_data.shape)\n",
    "print(new_joints_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMASS\n",
    "poses: (frames, 156)<br>\n",
    "  gender: ()<br>\n",
    "  mocap_framerate: ()<br>\n",
    "  betas: (16,)<br>\n",
    "  marker_data: (frames, marker_labels, 3)<br>\n",
    "  dmpls: (frames, 8)<br>\n",
    "  marker_labels: (marker_labels,)<br>\n",
    "  trans: (frames, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amass_data1 = np.load('AMASS\\ATUSquat_sync_poses.npz')\n",
    "amass_data2 = np.load('AMASS\\dribble_kick_sync_poses.npz')\n",
    "amass_data3 = np.load('AMASS\\shoulders_poses.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------amass_data1------------------------\n",
      "  poses: (257, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (257, 67, 3)\n",
      "  dmpls: (257, 8)\n",
      "  marker_labels: (67,)\n",
      "  trans: (257, 3)\n",
      "------------------------amass_data2------------------------\n",
      "  poses: (351, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (351, 65, 3)\n",
      "  dmpls: (351, 8)\n",
      "  marker_labels: (65,)\n",
      "  trans: (351, 3)\n",
      "------------------------amass_data3------------------------\n",
      "  poses: (282, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (282, 86, 3)\n",
      "  dmpls: (282, 8)\n",
      "  marker_labels: (86,)\n",
      "  trans: (282, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------amass_data1------------------------\")\n",
    "for array_name in amass_data1.files:\n",
    "    array_shape = amass_data1[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")\n",
    "print(\"------------------------amass_data2------------------------\")\n",
    "for array_name in amass_data2.files:\n",
    "    array_shape = amass_data2[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")\n",
    "print(\"------------------------amass_data3------------------------\")\n",
    "for array_name in amass_data3.files:\n",
    "    array_shape = amass_data3[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MotionX2HumanML3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vchoutas/smplx/tree/main/transfer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Motion-X的加载方式，其中'root_orient'和'pose_body'合起来维度是66，暂且认为是new_joints信息。<br>\n",
    "目前无法转换为new_joint_vecs的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# read motion and save as smplx representation\n",
    "motion = np.load('motion_data/smplx_322/000001.npy')\n",
    "motion = torch.tensor(motion).float()\n",
    "motion_parms = {\n",
    "            'root_orient': motion[:, :3],  # controls the global root orientation\n",
    "            'pose_body': motion[:, 3:3+63],  # controls the body\n",
    "            'pose_hand': motion[:, 66:66+90],  # controls the finger articulation\n",
    "            'pose_jaw': motion[:, 66+90:66+93],  # controls the yaw pose\n",
    "            'face_expr': motion[:, 159:159+50],  # controls the face expression\n",
    "            'face_shape': motion[:, 209:209+100],  # controls the face shape\n",
    "            'trans': motion[:, 309:309+3],  # controls the global body position\n",
    "            'betas': motion[:, 312:],  # controls the body shape. Body shape is static\n",
    "        }\n",
    "\n",
    "# read text labels\n",
    "semantic_text = np.loadtxt('semantic_labels/000001.npy')     # semantic labels "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
