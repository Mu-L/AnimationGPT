{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-X\n",
    "face_motion_data: shape = (frames, 153)<br>\n",
    "motion_data: shape = (frames, 322)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## face_motion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_motion_data1 = np.load('face_motion_data/000000_clip0000.npy')\n",
    "face_motion_data2 = np.load('face_motion_data/000.npy')\n",
    "face_motion_data3 = np.load('face_motion_data/001.npy')\n",
    "face_motion_data4 = np.load('face_motion_data/000002_clip0000.npy')\n",
    "face_motion_data5 = np.load('face_motion_data/airplane_pass_1.npy')\n",
    "face_motion_data6 = np.load('face_motion_data/gamecontroller_play_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 153)\n",
      "(450, 153)\n",
      "(401, 153)\n",
      "(69, 153)\n",
      "(149, 153)\n",
      "(146, 153)\n"
     ]
    }
   ],
   "source": [
    "print(face_motion_data1.shape)\n",
    "print(face_motion_data2.shape)\n",
    "print(face_motion_data3.shape)\n",
    "print(face_motion_data4.shape)\n",
    "print(face_motion_data5.shape)\n",
    "print(face_motion_data6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## motion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data1 = np.load('motion_data/32_Form_Tai_Chi_Demonstration_Master_Form3_Single_Whip_Left.npy')\n",
    "motion_data2 = np.load('motion_data/32_Form_Tai_Chi_Demonstration_Master_Form3_Single_Whip_Left2.npy')\n",
    "motion_data3 = np.load('motion_data/Dance_Break_3_Step.npy')\n",
    "motion_data4 = np.load('motion_data/Dance_Krump_Arm_Swing_clip_1.npy')\n",
    "motion_data5 = np.load('motion_data/Ways_To_Catch_360.npy')\n",
    "motion_data6 = np.load('motion_data/Ways_To_Wake_Up_Fall_Out_Of_Bed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 322)\n",
      "(10, 322)\n",
      "(82, 322)\n",
      "(300, 322)\n",
      "(117, 322)\n",
      "(37, 322)\n"
     ]
    }
   ],
   "source": [
    "print(motion_data1.shape)\n",
    "print(motion_data2.shape)\n",
    "print(motion_data3.shape)\n",
    "print(motion_data4.shape)\n",
    "print(motion_data5.shape)\n",
    "print(motion_data6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumanML3D & soul_v3\n",
    "new_joint_vecs_data: shape = (frames, 263)<br>\n",
    "263 is motion features extracted using motion_representation.ipynb<br>\n",
    "more infomation about dim-263: https://github.com/EricGuo5513/HumanML3D/issues/26\n",
    "<br>\n",
    "new_joints_data: shape = (frames, 22, 3)<br>\n",
    "<br>\n",
    "\n",
    "[bs, seqlen, 263/251] HumanML/KIT<br>\n",
    "- ./new_joint_vecs.rar //从3D运动位置提取旋转不变特征和旋转特征向量。\n",
    "- ./new_joints.rar //3D 运动位置。\n",
    "- ./Mean.npy //new_joint_vecs中所有数据的平均值\n",
    "- ./Std.npy //new_joint_vecs中所有数据的标准差\n",
    "<br>\n",
    "\n",
    "**SMPL有23个关节和一个根关节，而 HumanML3D 有 22 个关节：SMPL 的前 22 个关节（第23和第24关节是手关节）**<br>\n",
    "- root_rot_velocity (B, seq_len, 1) = 1\n",
    "- root_linear_velocity (B, seq_len,2) = 2\n",
    "- root_y (B, seq_len, 1) = 1\n",
    "- ric_data (B, seq_len, (joint_num - 1)*3) = 63\n",
    "- rot_data (B, seq_len, (joint_num - 1)*6) = 126\n",
    "- local_velocity (B, seq_len,joint_num*3) = 66\n",
    "- foot contact (B, seq_len, 4) = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_joint_vecs_data = np.load('..\\\\..\\\\algorithm\\HumanML3D\\HumanML3D\\\\new_joint_vecs\\\\012314.npy')\n",
    "new_joints_data = np.load('..\\\\..\\\\algorithm\\HumanML3D\\HumanML3D\\\\new_joints\\\\012314.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 263)\n",
      "(170, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "print(new_joint_vecs_data.shape)\n",
    "print(new_joints_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_joint_vecs_data = np.load('soul_v3\\\\new_joint_vecs\\SKR_250_600361.npy')\n",
    "new_joints_data = np.load('soul_v3\\\\new_joints\\SKR_250_600361.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169, 263)\n",
      "(169, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "print(new_joint_vecs_data.shape)\n",
    "print(new_joints_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMASS\n",
    "poses: (frames, 156)<br>\n",
    "  gender: ()<br>\n",
    "  mocap_framerate: ()<br>\n",
    "  betas: (16,)<br>\n",
    "  marker_data: (frames, marker_labels, 3)<br>\n",
    "  dmpls: (frames, 8)<br>\n",
    "  marker_labels: (marker_labels,)<br>\n",
    "  trans: (frames, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amass_data1 = np.load('AMASS\\ATUSquat_sync_poses.npz')\n",
    "amass_data2 = np.load('AMASS\\dribble_kick_sync_poses.npz')\n",
    "amass_data3 = np.load('AMASS\\shoulders_poses.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------amass_data1------------------------\n",
      "  poses: (257, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (257, 67, 3)\n",
      "  dmpls: (257, 8)\n",
      "  marker_labels: (67,)\n",
      "  trans: (257, 3)\n",
      "------------------------amass_data2------------------------\n",
      "  poses: (351, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (351, 65, 3)\n",
      "  dmpls: (351, 8)\n",
      "  marker_labels: (65,)\n",
      "  trans: (351, 3)\n",
      "------------------------amass_data3------------------------\n",
      "  poses: (282, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (282, 86, 3)\n",
      "  dmpls: (282, 8)\n",
      "  marker_labels: (86,)\n",
      "  trans: (282, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------amass_data1------------------------\")\n",
    "for array_name in amass_data1.files:\n",
    "    array_shape = amass_data1[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")\n",
    "print(\"------------------------amass_data2------------------------\")\n",
    "for array_name in amass_data2.files:\n",
    "    array_shape = amass_data2[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")\n",
    "print(\"------------------------amass_data3------------------------\")\n",
    "for array_name in amass_data3.files:\n",
    "    array_shape = amass_data3[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MotionX2HumanML3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vchoutas/smplx/tree/main/transfer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Motion-X的加载方式，其中'root_orient'和'pose_body'合起来维度是66，暂且认为是new_joints信息。<br>\n",
    "目前无法转换为new_joint_vecs的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# read motion and save as smplx representation\n",
    "motion = np.load('motion_data/smplx_322/000001.npy')\n",
    "motion = torch.tensor(motion).float()\n",
    "motion_parms = {\n",
    "            'root_orient': motion[:, :3],  # controls the global root orientation\n",
    "            'pose_body': motion[:, 3:3+63],  # controls the body\n",
    "            'pose_hand': motion[:, 66:66+90],  # controls the finger articulation\n",
    "            'pose_jaw': motion[:, 66+90:66+93],  # controls the yaw pose\n",
    "            'face_expr': motion[:, 159:159+50],  # controls the face expression\n",
    "            'face_shape': motion[:, 209:209+100],  # controls the face shape\n",
    "            'trans': motion[:, 309:309+3],  # controls the global body position\n",
    "            'betas': motion[:, 312:],  # controls the body shape. Body shape is static\n",
    "        }\n",
    "\n",
    "# read text labels\n",
    "semantic_text = np.loadtxt('semantic_labels/000001.npy')     # semantic labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Motion-X to AMASS\n",
    "2. AMASS to HumanML3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-X——AMASS\n",
    "  'pose_body': motion[:, 3:3+63] + 'pose_jaw': motion[:, 66+90:66+93] + 'pose_hand': motion[:, 66:66+90]——poses: (frames, 156)<br>\n",
    "  gender: ()<br>\n",
    "  mocap_framerate: ()<br>\n",
    "  'betas': motion[:, 312:]【322-312=10个，还缺6个】——betas: (16,)<br>\n",
    "  marker_data: (frames, marker_labels, 3)<br>\n",
    "  dmpls: (frames, 8)<br>\n",
    "  marker_labels: (marker_labels,)<br>\n",
    "  'trans': motion[:, 309:309+3]——trans: (frames, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "pelvis_joint_name = \"m_avg_Pelvis\"\n",
    "\n",
    "# 获取映射表\n",
    "mapping_table = pd.read_csv(\"keys_mapping.csv\")\n",
    "mapping_table.set_index('fbx_name', inplace=True)\n",
    "\n",
    "# mapping_table\n",
    "def get_changed_extention_abs_path(abs_path,ext):\n",
    "    return os.path.splitext(abs_path)[0]+\".\"+ext.replace(\".\",\"\")\n",
    "\n",
    "def mx2amass_npz(mx_path, mapping_table):\n",
    "\n",
    "    with open(mx_path, 'r', encoding='utf-8') as fp:\n",
    "        data = np.load(fp)\n",
    "        # frame是data的第一个维度\n",
    "        frame_length = data.shape[0]\n",
    "\n",
    "        # 通用\n",
    "        gender = 'male'                         # 性别\n",
    "        mocap_framerate = 30                    # 帧率\n",
    "        betas = np.zeros(16)                    # 形状参数，默认为0\n",
    "        dmpls = np.zeros((frame_length, 8))     # 软组织系数，默认为0\n",
    "\n",
    "        # 更新\n",
    "        trans = np.zeros((frame_length, 3))     # 全局平移\n",
    "        poses = np.zeros((frame_length, 156))   # 姿势参数\n",
    "\n",
    "        # 处理每帧数据\n",
    "        for frame_data in data[\"frame_sequence\"]:\n",
    "            # 第几帧索引\n",
    "            frame_index = frame_data[\"frame_number\"]\n",
    "\n",
    "            pelvis_position = np.zeros(3)\n",
    "\n",
    "            joint_positions = {}\n",
    "            joint_rotations = {}\n",
    "\n",
    "            # 处理关节点数据\n",
    "            for joint_data in frame_data[\"joint_position\"]:\n",
    "\n",
    "                joint_name = joint_data[\"joint_name\"]\n",
    "                # 尝试获取当前关节序号\n",
    "\n",
    "                if joint_name not in mapping_table.index:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                order = mapping_table.loc[joint_name][\"order\"]\n",
    "                joint_positions[order] = np.array([joint_data[\"position_x\"],\n",
    "                                                -joint_data[\"position_z\"],\n",
    "                                                joint_data[\"position_y\"]])\n",
    "\n",
    "                if joint_name == pelvis_joint_name:\n",
    "                    pelvis_position = joint_positions[order]\n",
    "\n",
    "                # 获取欧拉角\n",
    "                euler_angles = np.array([joint_data[\"rotation_x\"],\n",
    "                                        joint_data[\"rotation_y\"],\n",
    "                                        joint_data[\"rotation_z\"]])\n",
    "\n",
    "                euler_angles = (euler_angles + 180) % 360 - 180\n",
    "                # 将欧拉角转换为轴角表示法\n",
    "                joint_rotations[order] = Rotation.from_euler('xyz', euler_angles, degrees=True).as_rotvec()\n",
    "                        \n",
    "\n",
    "                \n",
    "                \n",
    "            for order in joint_rotations.keys():\n",
    "                poses[frame_index, order * 3:(order + 1) * 3] = joint_rotations[order]\n",
    "\n",
    "            # 计算m_avg_Pelvis和m_avg_root累计的位置变化\n",
    "            \n",
    "            trans[frame_index, :] = pelvis_position * 0.01\n",
    "\n",
    "    # 将数据保存为AMASS格式（NPZ文件）\n",
    "    np.savez(get_changed_extention_abs_path(json_path, \".npz\"), trans=trans, gender=gender, mocap_framerate=mocap_framerate, betas=betas, dmpls=dmpls, poses=poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def convert_to_amass_npz(npy_path, npz_path, mapping_table):\n",
    "    # Read motion data from npy file\n",
    "    motion = np.load(npy_path)\n",
    "    motion = torch.tensor(motion).float()\n",
    "\n",
    "    # Extract motion parameters\n",
    "    motion_parms = {\n",
    "        'root_orient': motion[:, :3],\n",
    "        'pose_body': motion[:, 3:3+63],\n",
    "        'pose_hand': motion[:, 66:66+90],\n",
    "        'trans': motion[:, 309:309+3],\n",
    "        'betas': motion[:, 312:],\n",
    "    }\n",
    "\n",
    "    # Create amass format data\n",
    "    frame_length = motion.shape[0]\n",
    "    gender = b'male'\n",
    "    mocap_framerate = 30\n",
    "    betas = np.zeros(16)\n",
    "    dmpls = np.zeros((frame_length, 8))\n",
    "    poses = np.zeros((frame_length, 156))\n",
    "    trans = np.zeros((frame_length, 3))\n",
    "\n",
    "    # Process each frame\n",
    "    for frame_index in range(frame_length):\n",
    "        pelvis_position = motion_parms['trans'][frame_index].numpy() * 0.01\n",
    "\n",
    "        joint_rotations = {}\n",
    "        for order, joint_data in mapping_table.iterrows():\n",
    "            joint_name = joint_data.name  # Get the keypoint name\n",
    "            if joint_name not in motion_parms:\n",
    "                continue\n",
    "\n",
    "            rotation_angles = motion_parms[joint_name][frame_index].numpy()\n",
    "            joint_rotations[order] = Rotation.from_euler('xyz', rotation_angles, degrees=True).as_rotvec()\n",
    "\n",
    "        # Combine pose_body and pose_hand into poses\n",
    "        poses[frame_index, :63] = motion_parms['pose_body'][frame_index].numpy()\n",
    "        poses[frame_index, 63:153] = motion_parms['pose_hand'][frame_index].numpy()\n",
    "\n",
    "        for order in joint_rotations.keys():\n",
    "            poses[frame_index, 153 + order * 3:156 + order * 3] = joint_rotations[order]\n",
    "\n",
    "        trans[frame_index, :] = pelvis_position\n",
    "\n",
    "    # Save data in amass format (NPZ file)\n",
    "    np.savez(npz_path, poses=poses, gender=gender, mocap_framerate=mocap_framerate, betas=betas, dmpls=dmpls, trans=trans)\n",
    "\n",
    "# Example usage\n",
    "npy_file_path = 'motion_data\\Dance_Break_3_Step.npy'\n",
    "npz_output_path = 'motion_data\\Dance_Break_3_Step_amassTest.npz'\n",
    "keys_mapping_table = pd.read_csv(\"json2amass_by_syan\\keys_mapping.csv\")\n",
    "keys_mapping_table.set_index('fbx_name', inplace=True)\n",
    "\n",
    "convert_to_amass_npz(npy_file_path, npz_output_path, keys_mapping_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([b'RBSH', b'LFTHI', b'RBHD', b'RPSI', b'LELBIN', b'LBSH', b'RTHMB', b'RFRM2IN', b'LFIN', b'RFIN', b'LTHMB', b'LOWR', b'LFRM2', b'LELB', b'LBCEP', b'LRSTBEEF', b'LSHO', b'LKNE', b'LBUST', b'LTOE', b'LMT1', b'LFTHIIN', b'MBLLY', b'STRN', b'RKNI', b'RCHEECK', b'RFHD', b'RANK', b'RFTHIIN', b'RKNE', b'RFTHI', b'RBUSTLO', b'RMT5', b'RSHO', b'RRSTBEEF', b'RBCEP', b'RELB', b'RFRM2', b'ROWR', b'RTHI', b'RTIB', b'RASI', b'LHEE', b'LASI', b'LNWST', b'RBUM', b'LPSI', b'RHEE', b'LTIB', b'LCHEECK', b'LFHD', b'LANK', b'RBAK', b'C7', b'LBHD', b'LTHI', b'LBUM', b'LBAK', b'T8', b'RELBIN', b'CLAV', b'RTOE', b'LMT5', b'LKNI', b'RMT1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([b'LMWT', b'LFRM', b'LSHN', b'LELB', b'LUPA', b'LANK', b'LSHO', b'LKNE', b'LFHD', b'RMT5', b'RSCAP', b'LBUM', b'RPSI', b'RBUM', b'RKNE', b'LTIB', b'RFRM2', b'RMT1', b'LSHNIN', b'LBUST', b'LBAK', b'RBAK', b'RHEE', b'LBHD', b'LHEE', b'LBSH', b'LSCAP', b'RTIBIN', b'RBSH', b'LRSTBEEF', b'LMT1', b'LTOE', b'ROWR', b'LTHMB', b'RTOE', b'RRSTBEEF', b'RBTHI', b'LOWR', b'LBTHI', b'LIWR', b'LCHEECK', b'C7', b'RFRMIN', b'RIWR', b'LTHILO', b'MBLLY', b'RSHO', b'RTHI', b'RFTHI', b'RBUST', b'RFHD', b'STRN', b'RELB', b'RFTHIIN', b'LMT5', b'RFRM', b'RUPA2', b'RBHD', b'LKNI', b'RTIB', b'RSHN', b'RANK', b'RCHEECK', b'RNWST', b'LFTHI', b'RFRM2IN', b'RTHILO', b'LASI', b'LBUSTLO', b'RFIN', b'RKNI', b'RASI', b'CLAV', b'LELBIN', b'RCLAV', b'LBCEP', b'LFRM2IN', b'LNECK', b'LFTHIIN', b'RTHMB', b'RNECK', b'LNWST', b'LFIN', b'LPSI', b'T8', b'LCLAV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def convert_to_amass_npz(npy_path, npz_path, mapping_table):\n",
    "    # Read motion data from npy file\n",
    "    motion = np.load(npy_path)\n",
    "    motion = torch.tensor(motion).float()\n",
    "\n",
    "    # Extract motion parameters\n",
    "    motion_parms = {\n",
    "        'root_orient': motion[:, :3],\n",
    "        'pose_body': motion[:, 3:3+63],\n",
    "        'pose_hand': motion[:, 66:66+90],\n",
    "        'trans': motion[:, 309:309+3],\n",
    "        'betas': motion[:, 312:],\n",
    "    }\n",
    "\n",
    "    # Create amass format data\n",
    "    frame_length = motion.shape[0]\n",
    "    gender = b'male'\n",
    "    mocap_framerate = 30\n",
    "    betas = np.zeros(16)\n",
    "    dmpls = np.zeros((frame_length, 8))\n",
    "    poses = np.zeros((frame_length, 156))\n",
    "    trans = np.zeros((frame_length, 3))\n",
    "\n",
    "    # Process each frame\n",
    "    for frame_index in range(frame_length):\n",
    "        pelvis_position = motion_parms['trans'][frame_index].numpy() * 0.01\n",
    "\n",
    "        joint_rotations = {}\n",
    "        for order, joint_data in mapping_table.iterrows():\n",
    "            joint_name = joint_data.name  # Get the keypoint name\n",
    "            if joint_name not in motion_parms:\n",
    "                continue\n",
    "\n",
    "            rotation_angles = motion_parms[joint_name][frame_index].numpy()\n",
    "            joint_rotations[order] = Rotation.from_euler('xyz', rotation_angles, degrees=True).as_rotvec()\n",
    "\n",
    "        # Combine pose_body and pose_hand into poses\n",
    "        poses[frame_index, :63] = motion_parms['pose_body'][frame_index].numpy()\n",
    "        poses[frame_index, 63:153] = motion_parms['pose_hand'][frame_index].numpy()\n",
    "\n",
    "        for order in joint_rotations.keys():\n",
    "            poses[frame_index, 153 + order * 3:156 + order * 3] = joint_rotations[order]\n",
    "\n",
    "        trans[frame_index, :] = pelvis_position\n",
    "\n",
    "    # Read marker labels\n",
    "    marker_labels = [b'RBSH', b'LFTHI', b'RBHD', b'RPSI', b'LELBIN', b'LBSH', b'RTHMB', b'RFRM2IN', b'LFIN', b'RFIN', b'LTHMB', b'LOWR', b'LFRM2', b'LELB', b'LBCEP', b'LRSTBEEF', b'LSHO', b'LKNE', b'LBUST', b'LTOE', b'LMT1', b'LFTHIIN', b'MBLLY', b'STRN', b'RKNI', b'RCHEECK', b'RFHD', b'RANK', b'RFTHIIN', b'RKNE', b'RFTHI', b'RBUSTLO', b'RMT5', b'RSHO', b'RRSTBEEF', b'RBCEP', b'RELB', b'RFRM2', b'ROWR', b'RTHI', b'RTIB', b'RASI', b'LHEE', b'LASI', b'LNWST', b'RBUM', b'LPSI', b'RHEE', b'LTIB', b'LCHEECK', b'LFHD', b'LANK', b'RBAK', b'C7', b'LBHD', b'LTHI', b'LBUM', b'LBAK', b'T8', b'RELBIN', b'CLAV', b'RTOE', b'LMT5', b'LKNI', b'RMT1']\n",
    "\n",
    "    # Create random marker_data (replace with actual data if available)\n",
    "    marker_data = np.random.rand(frame_length, len(marker_labels), 3)\n",
    "\n",
    "    # Save data in amass format (NPZ file)\n",
    "    np.savez(npz_path, poses=poses, gender=gender, mocap_framerate=mocap_framerate,\n",
    "             betas=betas, marker_data=marker_data,  dmpls=dmpls, marker_labels=marker_labels, trans=trans)\n",
    "\n",
    "# Example usage\n",
    "npy_file_path = 'motion_data\\Dance_Break_3_Step.npy'\n",
    "npz_output_path = 'motion_data\\Dance_Break_3_Step_amassTest.npz'\n",
    "keys_mapping_table = pd.read_csv(\"json2amass_by_syan\\keys_mapping.csv\")\n",
    "keys_mapping_table.set_index('fbx_name', inplace=True)\n",
    "\n",
    "convert_to_amass_npz(npy_file_path, npz_output_path, keys_mapping_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------amass_test1------------------------\n",
      "  poses: (82, 156)\n",
      "  gender: ()\n",
      "  mocap_framerate: ()\n",
      "  betas: (16,)\n",
      "  marker_data: (82, 65, 3)\n",
      "  dmpls: (82, 8)\n",
      "  marker_labels: (65,)\n",
      "  trans: (82, 3)\n"
     ]
    }
   ],
   "source": [
    "amass_test1 = np.load('motion_data\\Dance_Break_3_Step_amassTest.npz')\n",
    "print(\"------------------------amass_test1------------------------\")\n",
    "for array_name in amass_test1.files:\n",
    "    array_shape = amass_test1[array_name].shape\n",
    "    print(f\"  {array_name}: {array_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'S:\\\\【数字人数据集】\\\\Motion-X\\\\motion_data\\\\npz\\\\smplx_322\\\\aist\\\\subset_0000\\\\Dance_Break.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32416\\3063287295.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mkeys_mapping_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fbx_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mconvert_folder_to_amass_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpy_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpz_output_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys_mapping_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32416\\3063287295.py\u001b[0m in \u001b[0;36mconvert_folder_to_amass_npz\u001b[1;34m(folder_path, output_folder, mapping_table)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Convert npy to npz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mconvert_to_amass_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpy_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpz_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_amass_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpy_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpz_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32416\\3063287295.py\u001b[0m in \u001b[0;36mconvert_to_amass_npz\u001b[1;34m(npy_path, npz_path, mapping_table)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m# Save data in amass format (NPZ file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     np.savez(npz_path, poses=poses, gender=gender, mocap_framerate=mocap_framerate,\n\u001b[1;32m---> 74\u001b[1;33m              betas=betas, marker_data=marker_data,  dmpls=dmpls, marker_labels=marker_labels, trans=trans)\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavez\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fyy\\.conda\\envs\\ptl\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msavez\u001b[1;34m(file, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \"\"\"\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[0m_savez\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fyy\\.conda\\envs\\ptl\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[1;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZIP_STORED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m     \u001b[0mzipf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnamedict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fyy\\.conda\\envs\\ptl\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mzipfile_factory\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allowZip64'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fyy\\.conda\\envs\\ptl\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'S:\\\\【数字人数据集】\\\\Motion-X\\\\motion_data\\\\npz\\\\smplx_322\\\\aist\\\\subset_0000\\\\Dance_Break.npz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def convert_folder_to_amass_npz(folder_path, output_folder, mapping_table):\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get all npy files in the folder and its subfolders\n",
    "    npy_files = [os.path.join(folder_path, root, file) for root, dirs, files in os.walk(folder_path) for file in files if file.endswith(\".npy\")]\n",
    "\n",
    "    for npy_file_path in npy_files:\n",
    "        # Generate output npz file path\n",
    "        npz_file_path = os.path.join(output_folder, os.path.relpath(npy_file_path, folder_path).replace(\".npy\", \".npz\"))\n",
    "\n",
    "        # Convert npy to npz\n",
    "        convert_to_amass_npz(npy_file_path, npz_file_path, mapping_table)\n",
    "\n",
    "def convert_to_amass_npz(npy_path, npz_path, mapping_table):\n",
    "    # Read motion data from npy file\n",
    "    motion = np.load(npy_path)\n",
    "    motion = torch.tensor(motion).float()\n",
    "\n",
    "    # Extract motion parameters\n",
    "    motion_parms = {\n",
    "        'root_orient': motion[:, :3],\n",
    "        'pose_body': motion[:, 3:3+63],\n",
    "        'pose_hand': motion[:, 66:66+90],\n",
    "        'trans': motion[:, 309:309+3],\n",
    "        'betas': motion[:, 312:],\n",
    "    }\n",
    "\n",
    "    # Create amass format data\n",
    "    frame_length = motion.shape[0]\n",
    "    gender = b'male'\n",
    "    mocap_framerate = 30\n",
    "    betas = np.zeros(16)\n",
    "    dmpls = np.zeros((frame_length, 8))\n",
    "    poses = np.zeros((frame_length, 156))\n",
    "    trans = np.zeros((frame_length, 3))\n",
    "\n",
    "    # Process each frame\n",
    "    for frame_index in range(frame_length):\n",
    "        pelvis_position = motion_parms['trans'][frame_index].numpy() * 0.01\n",
    "\n",
    "        joint_rotations = {}\n",
    "        for order, joint_data in mapping_table.iterrows():\n",
    "            joint_name = joint_data.name  # Get the keypoint name\n",
    "            if joint_name not in motion_parms:\n",
    "                continue\n",
    "\n",
    "            rotation_angles = motion_parms[joint_name][frame_index].numpy()\n",
    "            joint_rotations[order] = Rotation.from_euler('xyz', rotation_angles, degrees=True).as_rotvec()\n",
    "\n",
    "        # Combine pose_body and pose_hand into poses\n",
    "        poses[frame_index, :63] = motion_parms['pose_body'][frame_index].numpy()\n",
    "        poses[frame_index, 63:153] = motion_parms['pose_hand'][frame_index].numpy()\n",
    "\n",
    "        for order in joint_rotations.keys():\n",
    "            poses[frame_index, 153 + order * 3:156 + order * 3] = joint_rotations[order]\n",
    "\n",
    "        trans[frame_index, :] = pelvis_position\n",
    "\n",
    "    # Read marker labels\n",
    "    marker_labels = [b'RBSH', b'LFTHI', b'RBHD', b'RPSI', b'LELBIN', b'LBSH', b'RTHMB', b'RFRM2IN', b'LFIN', b'RFIN', b'LTHMB', b'LOWR', b'LFRM2', b'LELB', b'LBCEP', b'LRSTBEEF', b'LSHO', b'LKNE', b'LBUST', b'LTOE', b'LMT1', b'LFTHIIN', b'MBLLY', b'STRN', b'RKNI', b'RCHEECK', b'RFHD', b'RANK', b'RFTHIIN', b'RKNE', b'RFTHI', b'RBUSTLO', b'RMT5', b'RSHO', b'RRSTBEEF', b'RBCEP', b'RELB', b'RFRM2', b'ROWR', b'RTHI', b'RTIB', b'RASI', b'LHEE', b'LASI', b'LNWST', b'RBUM', b'LPSI', b'RHEE', b'LTIB', b'LCHEECK', b'LFHD', b'LANK', b'RBAK', b'C7', b'LBHD', b'LTHI', b'LBUM', b'LBAK', b'T8', b'RELBIN', b'CLAV', b'RTOE', b'LMT5', b'LKNI', b'RMT1']\n",
    "\n",
    "    # Create random marker_data (replace with actual data if available)\n",
    "    marker_data = np.random.rand(frame_length, len(marker_labels), 3)\n",
    "\n",
    "        # Save data in amass format (NPZ file)\n",
    "    os.makedirs(os.path.dirname(npz_path), exist_ok=True)  # Create necessary directories\n",
    "    np.savez(npz_path, poses=poses, gender=gender, mocap_framerate=mocap_framerate,\n",
    "             betas=betas, marker_data=marker_data, dmpls=dmpls, marker_labels=marker_labels, trans=trans)\n",
    "\n",
    "# Example usage\n",
    "npy_folder_path = 'S:\\【数字人数据集】\\Motion-X\\motion_data'  # Change this to your actual folder path\n",
    "npz_output_folder = 'S:\\【数字人数据集】\\Motion-X\\motion_data\\\\npz'  # Change this to your desired output folder\n",
    "\n",
    "keys_mapping_table = pd.read_csv(\"json2amass_by_syan/keys_mapping.csv\")\n",
    "keys_mapping_table.set_index('fbx_name', inplace=True)\n",
    "\n",
    "convert_folder_to_amass_npz(npy_folder_path, npz_output_folder, keys_mapping_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
